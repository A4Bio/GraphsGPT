{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Graph Clustering",
   "metadata": {
    "collapsed": false
   },
   "id": "e4f62e69f34c204b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import umap\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Configurations",
   "metadata": {
    "collapsed": false
   },
   "id": "dc1752feceb112e7"
  },
  {
   "cell_type": "code",
   "source": [
    "model_name_or_path = \"DaizeDong/GraphsGPT-8W\"\n",
    "smiles_file = \"../../data/examples/zinc_example.txt\"\n",
    "\n",
    "batch_size = 1024\n",
    "vis_sample_num = 1024 * 32\n",
    "vis_save_dir = \"./clustering_results\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af175bf140dbdc73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load SMILES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6c824797ec1f94b"
  },
  {
   "cell_type": "code",
   "source": [
    "with open(smiles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    smiles_list = f.readlines()\n",
    "smiles_list = [smiles.removesuffix(\"\\n\") for smiles in smiles_list]\n",
    "\n",
    "print(f\"Total SMILES loaded: {len(smiles_list)}\")\n",
    "for i in range(10):\n",
    "    print(f\"Example SMILES {i}: {smiles_list[i]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502345a4094807d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Model Checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e185f38cdeed02a6"
  },
  {
   "cell_type": "code",
   "source": [
    "from models.graphsgpt.modeling_graphsgpt import GraphsGPTForCausalLM\n",
    "from data.tokenizer import GraphsGPTTokenizer\n",
    "\n",
    "model = GraphsGPTForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = GraphsGPTTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(model.state_dict().keys())\n",
    "print(f\"Total paramerters: {sum(x.numel() for x in model.parameters())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b79a2ec1f4064f8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b44805bddfd2d3"
  },
  {
   "cell_type": "code",
   "source": [
    "from utils.operations.operation_list import split_list_with_yield\n",
    "from utils.operations.operation_tensor import move_tensors_to_device\n",
    "\n",
    "# generate fingerprint tokens\n",
    "now_sample_num = 0\n",
    "all_fingerprint_tokens = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batched_smiles in split_list_with_yield(smiles_list, batch_size):\n",
    "        inputs = tokenizer.batch_encode(batched_smiles, return_tensors=\"pt\")\n",
    "        move_tensors_to_device(inputs, device)\n",
    "\n",
    "        fingerprint_tokens = model.encode_to_fingerprints(**inputs)  # (batch_size, num_fingerprints, hidden_dim)\n",
    "\n",
    "        # limit the number of samples\n",
    "        this_sample_num = fingerprint_tokens.shape[0]\n",
    "        append_sample_num = min(this_sample_num, vis_sample_num - now_sample_num)\n",
    "        if append_sample_num > 0:\n",
    "            now_sample_num += append_sample_num\n",
    "            all_fingerprint_tokens.append(fingerprint_tokens)\n",
    "        if append_sample_num < this_sample_num:\n",
    "            print(\"Max sample num reached, stopping forwarding.\")\n",
    "            break\n",
    "\n",
    "# fingerprint tokens to numpy\n",
    "all_fingerprint_tokens = torch.cat(all_fingerprint_tokens, dim=0)\n",
    "all_fingerprint_tokens = all_fingerprint_tokens.cpu().numpy()  # (vis_sample_num, num_fingerprints, hidden_dim)\n",
    "num_fingerprint_tokens = fingerprint_tokens.shape[1]\n",
    "print(f\"Number of samples is {all_fingerprint_tokens.shape[0]}\")\n",
    "print(f\"Number of fingerprints for each sample is {num_fingerprint_tokens}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed308d72158d4257",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UMAP Dimensionality Reduction\n",
    "\n",
    "For reference, [here](README-Clustering.md) are some hyperparameters for UMAP and HDBSCAN."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f77478ec8fd5eb"
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare features for UMAP\n",
    "features = {}\n",
    "\n",
    "if num_fingerprint_tokens > 1:  # per-fingerprint\n",
    "    for i in range(num_fingerprint_tokens):\n",
    "        features[f\"fp_{i}\"] = all_fingerprint_tokens[:, i, :]\n",
    "features[\"fp_all\"] = all_fingerprint_tokens.reshape(vis_sample_num, -1)  # aggregated fingerprints\n",
    "\n",
    "# start UMAP\n",
    "umap_features_for_clustering = {}\n",
    "umap_features_for_visualization = {}\n",
    "\n",
    "for key, value in tqdm(features.items(), desc=\"Computing UMAP features\"):\n",
    "    this_umap_features_for_clustering = umap.UMAP(\n",
    "        n_neighbors=100,  # bigger value --> more compact\n",
    "        min_dist=0.05,  # smaller value --> more compact\n",
    "        n_components=2\n",
    "    ).fit_transform(value)\n",
    "\n",
    "    this_umap_features_for_visualization = umap.UMAP(\n",
    "        n_neighbors=40, # same as above\n",
    "        min_dist=0.7,\n",
    "        n_components=2\n",
    "    ).fit_transform(value)\n",
    "\n",
    "    umap_features_for_clustering[key] = this_umap_features_for_clustering\n",
    "    umap_features_for_visualization[key] = this_umap_features_for_visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e7db31a093da85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HDBSCAN Clustering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d89033378f4dcb7"
  },
  {
   "cell_type": "code",
   "source": [
    "# HDBSCAN clustering\n",
    "cluster_labels = {}\n",
    "\n",
    "for key in tqdm(features.keys(), desc=\"Performing HDBSCAN clustering\"):\n",
    "    this_cluster_labels = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=32,  # bigger value --> less clusters\n",
    "        min_samples=48,  # bigger value --> less clusters & more noise\n",
    "        cluster_selection_epsilon=0.2,  # bigger value --> less noise\n",
    "        alpha=1.0,\n",
    "        gen_min_span_tree=True,\n",
    "    ).fit_predict(umap_features_for_clustering[key])\n",
    "\n",
    "    cluster_labels[key] = this_cluster_labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f70ae44831afab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization for Clusters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8fb733f31413090"
  },
  {
   "cell_type": "code",
   "source": [
    "for key in tqdm(features.keys(), desc=\"Performing visualization\"):\n",
    "    # features\n",
    "    noise_point_mask = (cluster_labels[key] == -1)\n",
    "    noise_sample_num = np.sum(noise_point_mask)\n",
    "    cluster_num = np.bincount(cluster_labels[key][~noise_point_mask]).shape[0]\n",
    "    cluster_centroids = []\n",
    "    for i in range(cluster_num):  # calculate for the cluster center\n",
    "        this_cluster_point_mask = (cluster_labels[key] == i)\n",
    "        this_cluster_point_feature = umap_features_for_visualization[key][this_cluster_point_mask]\n",
    "        this_cluster_centroid = np.mean(this_cluster_point_feature, axis=0)\n",
    "        cluster_centroids.append(this_cluster_centroid)\n",
    "\n",
    "    # get image\n",
    "    if not os.path.exists(vis_save_dir):\n",
    "        os.makedirs(vis_save_dir)\n",
    "\n",
    "    save_img_file = os.path.join(vis_save_dir, key + \".png\")\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.scatter(\n",
    "        umap_features_for_visualization[key][noise_point_mask][:, 0],\n",
    "        umap_features_for_visualization[key][noise_point_mask][:, 1],\n",
    "        c=\"#CCCCCC\",\n",
    "        label=\"noise\",\n",
    "        alpha=0.9,\n",
    "        s=16,\n",
    "        linewidths=0\n",
    "    )\n",
    "    ax.scatter(\n",
    "        umap_features_for_visualization[key][~noise_point_mask][:, 0],\n",
    "        umap_features_for_visualization[key][~noise_point_mask][:, 1],\n",
    "        c=cluster_labels[key][~noise_point_mask],\n",
    "        label=\"clusters\",\n",
    "        alpha=0.9,\n",
    "        s=16,\n",
    "        linewidths=0,\n",
    "        cmap=\"rainbow\"\n",
    "    )\n",
    "    for i, centroid in enumerate(cluster_centroids):  # Add text label at the cluster centroid\n",
    "        ax.text(\n",
    "            centroid[0],\n",
    "            centroid[1],\n",
    "            str(i),\n",
    "            color=\"black\",\n",
    "            fontsize=14,\n",
    "            weight='bold',\n",
    "            ha='center',\n",
    "            va='center'\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{key} (total cluster {cluster_num}) (total noise sample {noise_sample_num})\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(save_img_file, dpi=480, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fc42c202e28d240",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of Different Clusters\n",
    "\n",
    "We visualize the molecules in each clusters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558247a6e2ada704"
  },
  {
   "cell_type": "code",
   "source": [
    "from utils.io import delete_file_or_dir, create_dir, save_mol_png\n",
    "from utils.operations.operation_dict import reverse_dict\n",
    "\n",
    "# read bond dict\n",
    "bond_dict = tokenizer.bond_dict\n",
    "inverse_bond_dict = reverse_dict(bond_dict, aggregate_same_results=False)\n",
    "\n",
    "# visualize\n",
    "for key in tqdm(features.keys(), desc=\"Iterating over fingerprint features\"):\n",
    "    # features    \n",
    "    this_noise_point_mask = (cluster_labels[key] == -1)\n",
    "    this_cluster_num = np.bincount(cluster_labels[key][~this_noise_point_mask]).shape[0]\n",
    "\n",
    "    # get molecule images in different clusters\n",
    "    for i in tqdm(range(this_cluster_num), desc=\"Iterating over clusters\"):\n",
    "        this_cluster_indices_list = []\n",
    "        this_cluster_smiles_list = []\n",
    "\n",
    "        # molecule info\n",
    "        this_cluster_point_mask = (cluster_labels[key] == i)\n",
    "        this_cluster_point_id = np.arange(vis_sample_num)[this_cluster_point_mask].tolist()\n",
    "\n",
    "        # save mole image\n",
    "        mole_vis_save_dir = os.path.join(vis_save_dir, f\"moles_{key}\", f\"cluster_{i}\")\n",
    "        delete_file_or_dir(mole_vis_save_dir)\n",
    "        create_dir(mole_vis_save_dir)\n",
    "\n",
    "        save_img_cnt = 0\n",
    "        for index in this_cluster_point_id:\n",
    "            mol = tokenizer._convert_smiles_to_standard_molecule(smiles_list[index])\n",
    "\n",
    "            if mol is not None:\n",
    "                if save_img_cnt < 10:  # we visualize 10 samples at most for each cluster\n",
    "                    save_img_file = os.path.join(mole_vis_save_dir, f\"{index}.png\")\n",
    "                    save_mol_png(mol, save_img_file)\n",
    "                    save_img_cnt += 1\n",
    "                smiles = tokenizer._convert_molecule_to_standard_smiles(mol)\n",
    "                this_cluster_indices_list.append(index)\n",
    "                this_cluster_smiles_list.append(smiles)\n",
    "\n",
    "        # save mole SMILES\n",
    "        save_summary_file = os.path.join(mole_vis_save_dir, f\"summary.csv\")\n",
    "        with open(save_summary_file, \"w\") as f:\n",
    "            f.write(f\"index,smiles\\n\")\n",
    "            for j in range(len(this_cluster_indices_list)):\n",
    "                f.write(f\"{this_cluster_indices_list[j]},{smiles_list[j]}\\n\")\n",
    "\n",
    "print(f\"Visualization molecules saved to {vis_save_dir}.\")\n",
    "print(f\"Molecule SMILES in of each cluster saved to {vis_save_dir}.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b77765956e2cb114",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All done.\n",
    "You can check the saved files for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "496bd6f222365631"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e50d39af35672acc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
