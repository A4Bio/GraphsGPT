{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Graph Interpolation",
   "metadata": {
    "collapsed": false
   },
   "id": "bd3e245ebb0267d7"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e81a7c2d44475574",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Configurations",
   "metadata": {
    "collapsed": false
   },
   "id": "fe22cd11c9f18830"
  },
  {
   "cell_type": "code",
   "source": [
    "model_name_or_path = \"DaizeDong/GraphsGPT-8W\"\n",
    "smiles_file = \"../../data/examples/zinc_example.txt\"\n",
    "\n",
    "batch_size = 1024\n",
    "num_reference_moles = 1000\n",
    "num_interpolated_features = 1000\n",
    "interpolation_save_dir = \"./interpolation_results\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f5aeeb1c4a77ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load SMILES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf366744743e0b3a"
  },
  {
   "cell_type": "code",
   "source": [
    "with open(smiles_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    smiles_list = f.readlines()\n",
    "smiles_list = [smiles.removesuffix(\"\\n\") for smiles in smiles_list]\n",
    "\n",
    "print(f\"Total SMILES loaded: {len(smiles_list)}\")\n",
    "for i in range(10):\n",
    "    print(f\"Example SMILES {i}: {smiles_list[i]}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c32095433d920f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Model & Tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43c048d19faca6b9"
  },
  {
   "cell_type": "code",
   "source": [
    "from models.graphsgpt.modeling_graphsgpt import GraphsGPTForCausalLM\n",
    "from data.tokenizer import GraphsGPTTokenizer\n",
    "\n",
    "model = GraphsGPTForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = GraphsGPTTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(model.state_dict().keys())\n",
    "print(f\"Total paramerters: {sum(x.numel() for x in model.parameters())}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "215b5b8d9463549b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate Reference Molecules\n",
    "\n",
    "Results will be saved to the \"reference\" folder. (Empty image means generation failed)\n",
    "\n",
    "You need to select two molecules from them for the interpolation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62210aff5da53d2a"
  },
  {
   "cell_type": "code",
   "source": [
    "from utils.operations.operation_list import split_list_with_yield\n",
    "from utils.operations.operation_tensor import move_tensors_to_device\n",
    "from utils.operations.operation_dict import reverse_dict\n",
    "from utils.io import delete_file_or_dir, save_empty_png, save_mol_png\n",
    "\n",
    "# read bond dict\n",
    "bond_dict = tokenizer.bond_dict\n",
    "inverse_bond_dict = reverse_dict(bond_dict)\n",
    "\n",
    "# generate fingerprint tokens\n",
    "now_sample_num = 0\n",
    "all_fingerprint_tokens = []\n",
    "all_generated_results = []\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Generating for reference...\")\n",
    "with torch.no_grad():\n",
    "    for batched_smiles in split_list_with_yield(smiles_list, batch_size):\n",
    "        inputs = tokenizer.batch_encode(batched_smiles, return_tensors=\"pt\")\n",
    "        move_tensors_to_device(inputs, device)\n",
    "\n",
    "        fingerprint_tokens = model.encode_to_fingerprints(**inputs)  # (batch_size, num_fingerprints, hidden_dim)\n",
    "        generated_results: list = model.generate_from_fingerprints(\n",
    "            fingerprint_tokens=fingerprint_tokens,\n",
    "            bond_dict=bond_dict,\n",
    "            strict_generation=True,\n",
    "            max_atoms=None,\n",
    "            similarity_threshold=0.5,\n",
    "            check_first_node=True,\n",
    "            check_atom_valence=False,\n",
    "            fix_aromatic_bond=False,\n",
    "            use_cache=False,\n",
    "            save_failed=False,\n",
    "            show_progress=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # limit the number of samples\n",
    "        this_sample_num = fingerprint_tokens.shape[0]\n",
    "        append_sample_num = min(this_sample_num, num_reference_moles - now_sample_num)\n",
    "        if append_sample_num > 0:\n",
    "            now_sample_num += append_sample_num\n",
    "            all_fingerprint_tokens.append(fingerprint_tokens[:append_sample_num])\n",
    "            all_generated_results.extend(generated_results[:append_sample_num])\n",
    "        if append_sample_num < this_sample_num:\n",
    "            print(\"Max sample num reached, stopping forwarding.\")\n",
    "            break\n",
    "\n",
    "all_fingerprint_tokens = torch.cat(all_fingerprint_tokens, dim=0)\n",
    "num_fingerprint_tokens = fingerprint_tokens.shape[1]\n",
    "print(f\"Number of samples is {all_fingerprint_tokens.shape[0]}\")\n",
    "print(f\"Number of fingerprints for each sample is {num_fingerprint_tokens}\")\n",
    "\n",
    "# save generated results to disk as reference molecules\n",
    "success_cnt = 0\n",
    "invalid_cnt = 0\n",
    "fail_cnt = 0\n",
    "save_smiles_list = []\n",
    "\n",
    "delete_file_or_dir(os.path.join(interpolation_save_dir, \"references\"))\n",
    "os.makedirs(os.path.join(interpolation_save_dir, \"references\"))\n",
    "\n",
    "for i, result in tqdm(enumerate(all_generated_results), desc=\"Saving molecule images\"):\n",
    "    save_img_path = os.path.join(interpolation_save_dir, \"references\", f\"{i}.png\")\n",
    "\n",
    "    if result is not None:\n",
    "        input_ids = result[\"input_ids\"]\n",
    "        graph_position_ids_1 = result[\"graph_position_ids_1\"]\n",
    "        graph_position_ids_2 = result[\"graph_position_ids_2\"]\n",
    "        identifier_ids = result[\"identifier_ids\"]\n",
    "\n",
    "        mol = tokenizer._convert_token_tensors_to_molecule(input_ids, graph_position_ids_1, graph_position_ids_2, identifier_ids, inverse_bond_dict)\n",
    "\n",
    "        if mol is None:\n",
    "            save_smiles_list.append(None)\n",
    "            save_empty_png(save_img_path)\n",
    "            invalid_cnt += 1\n",
    "        else:\n",
    "            smiles = tokenizer._convert_molecule_to_standard_smiles(mol)\n",
    "            save_smiles_list.append(smiles)\n",
    "            save_mol_png(mol, save_img_path)\n",
    "            success_cnt += 1\n",
    "    else:\n",
    "        save_smiles_list.append(None)\n",
    "        save_empty_png(save_img_path)\n",
    "        fail_cnt += 1\n",
    "\n",
    "# save statistics\n",
    "with open(os.path.join(interpolation_save_dir, \"references\", \"count.txt\"), 'a') as f:\n",
    "    f.write(f\"Success count: {success_cnt}\\n\")\n",
    "    f.write(f\"Invalid count: {invalid_cnt}\\n\")\n",
    "    f.write(f\"Fail count: {fail_cnt}\\n\")\n",
    "\n",
    "with open(os.path.join(interpolation_save_dir, \"references\", \"smiles.txt\"), 'a') as f:\n",
    "    for smiles in save_smiles_list:\n",
    "        f.write(f\"{smiles}\\n\")\n",
    "\n",
    "print(f\"Results saved to {os.path.join(interpolation_save_dir, 'references')}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38eb5fc0750ad3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select Molecules to Perform Interpolation\n",
    "\n",
    "Select the indices of molecules for interpolation!\n",
    "\n",
    "It is better that selected molecules are generated successfully (corresponding images are not empty)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce5baee214469979"
  },
  {
   "cell_type": "code",
   "source": [
    "# You can change the index according to the results of molecules\n",
    "mole_index_1 = 81\n",
    "mole_index_2 = 89"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5aa1def334735f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate Interpolation Molecules\n",
    "\n",
    "Results will be saved to the folder named with the selected two molecule indices. \n",
    "\n",
    "We only save molecule images with different SMILES to save memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "585d7ca74d661c79"
  },
  {
   "cell_type": "code",
   "source": [
    "# get reference fingerprint tokens\n",
    "fingerprint_tokens_1 = all_fingerprint_tokens[mole_index_1].unsqueeze(0)  # (1, num_fingerprints, embed_size)\n",
    "fingerprint_tokens_2 = all_fingerprint_tokens[mole_index_2].unsqueeze(0)  # (1, num_fingerprints, embed_size)\n",
    "\n",
    "# linear interpolation\n",
    "interpolation_fingerprint_tokens = []\n",
    "\n",
    "interpolation_fingerprint_tokens.append(fingerprint_tokens_1)\n",
    "for i in range(num_interpolated_features):\n",
    "    alpha = (i + 1) / (num_interpolated_features + 1)\n",
    "    interpolated_tensors = torch.lerp(fingerprint_tokens_1, fingerprint_tokens_2, alpha)\n",
    "    interpolation_fingerprint_tokens.append(interpolated_tensors)\n",
    "interpolation_fingerprint_tokens.append(fingerprint_tokens_2)\n",
    "\n",
    "interpolation_fingerprint_tokens = torch.cat(interpolation_fingerprint_tokens, dim=0)\n",
    "\n",
    "# generate\n",
    "print(f\"Generating for interpolation...\")\n",
    "interpolation_generated_results: list = model.generate_from_fingerprints(\n",
    "    fingerprint_tokens=interpolation_fingerprint_tokens,\n",
    "    bond_dict=bond_dict,\n",
    "    strict_generation=True,\n",
    "    max_atoms=None,\n",
    "    similarity_threshold=0.5,\n",
    "    check_first_node=True,\n",
    "    check_atom_valence=False,\n",
    "    fix_aromatic_bond=False,\n",
    "    use_cache=False,\n",
    "    save_failed=False,\n",
    "    show_progress=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# save results\n",
    "success_cnt = 0\n",
    "invalid_cnt = 0\n",
    "fail_cnt = 0\n",
    "save_smiles_list = []\n",
    "last_smiles = None\n",
    "\n",
    "delete_file_or_dir(os.path.join(interpolation_save_dir, f\"{mole_index_1}--{mole_index_2}\"))\n",
    "os.makedirs(os.path.join(interpolation_save_dir, f\"{mole_index_1}--{mole_index_2}\"))\n",
    "\n",
    "for i, result in tqdm(enumerate(interpolation_generated_results), desc=\"Saving interpolation molecule images\"):\n",
    "    save_img_path = os.path.join(interpolation_save_dir, f\"{mole_index_1}--{mole_index_2}\", f\"{i}.png\")\n",
    "\n",
    "    if result is not None:\n",
    "        input_ids = result[\"input_ids\"]\n",
    "        graph_position_ids_1 = result[\"graph_position_ids_1\"]\n",
    "        graph_position_ids_2 = result[\"graph_position_ids_2\"]\n",
    "        identifier_ids = result[\"identifier_ids\"]\n",
    "\n",
    "        mol, smiles = tokenizer.decode(result)\n",
    "\n",
    "        if mol is None:\n",
    "            save_smiles_list.append(None)\n",
    "            invalid_cnt += 1\n",
    "        else:\n",
    "            save_smiles_list.append(smiles)\n",
    "            success_cnt += 1\n",
    "\n",
    "    else:\n",
    "        save_smiles_list.append(None)\n",
    "        fail_cnt += 1\n",
    "\n",
    "# save statistics\n",
    "with open(os.path.join(interpolation_save_dir, f\"{mole_index_1}--{mole_index_2}\", \"count.txt\"), 'a') as f:\n",
    "    f.write(f\"Success count: {success_cnt}\\n\")\n",
    "    f.write(f\"Invalid count: {invalid_cnt}\\n\")\n",
    "    f.write(f\"Fail count: {fail_cnt}\\n\")\n",
    "\n",
    "with open(os.path.join(interpolation_save_dir, f\"{mole_index_1}--{mole_index_2}\", \"smiles.txt\"), 'a') as f:\n",
    "    for smiles in save_smiles_list:\n",
    "        f.write(f\"{smiles}\\n\")\n",
    "\n",
    "print(f\"Results saved to {os.path.join(interpolation_save_dir, f'{mole_index_1}--{mole_index_2}')}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ded31e9b71301dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "All done.\n",
    "You can check the saved files for further analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f092f34e7efcf8c"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "668c65c2eb3860a5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
